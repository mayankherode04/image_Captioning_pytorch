{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22dccc56-3a95-44f2-b810-807c960af419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mayank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nltk\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK Tokenizer\n",
    "torch.manual_seed(42)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Hyperparameters\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# Custom Dataset\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, img_folder, captions_file, transform=None):\n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(captions_file, 'r') as f:\n",
    "            self.captions_data = json.load(f)\n",
    "        \n",
    "        self.images = list(self.captions_data.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_folder, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        caption = self.captions_data[img_name]\n",
    "        return image, caption\n",
    "\n",
    "# Encoder (CNN Feature Extractor)\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.fc(features)\n",
    "        return self.relu(features)\n",
    "\n",
    "# Decoder (LSTM for Caption Generation)\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions)\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        lstm_out, _ = self.lstm(inputs)\n",
    "        outputs = self.fc(lstm_out)\n",
    "        return outputs\n",
    "\n",
    "# Training Loop\n",
    "def train_model():\n",
    "    dataset = CocoDataset(\"images/\", \"captions.json\", transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    vocab_size = 1000  # Assume a fixed vocab size for simplicity\n",
    "    encoder = EncoderCNN(EMBED_SIZE).cuda()\n",
    "    decoder = DecoderRNN(EMBED_SIZE, HIDDEN_SIZE, vocab_size, NUM_LAYERS).cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=LEARNING_RATE)\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for i, (images, captions) in enumerate(data_loader):\n",
    "            images = images.cuda()\n",
    "            captions = captions.cuda()\n",
    "            \n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions[:, :-1])\n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].reshape(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i}/{len(data_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    torch.save(encoder.state_dict(), 'encoder.pth')\n",
    "    torch.save(decoder.state_dict(), 'decoder.pth')\n",
    "\n",
    "\n",
    "    train_model()\n",
    "    print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13e9f5-38a0-47ac-bb0e-627339a1d0af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
